{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708f416d-fbed-4441-8940-425d26a3b3a3",
   "metadata": {},
   "source": [
    "\n",
    "# EVALUATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf32f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"your-key\"\n",
    "openai_api_key=\"your-key\"\n",
    "result=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d463fe2-e8e0-4b80-b3d9-feb435a34e4d",
   "metadata": {},
   "source": [
    "### DEEPEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9253b-95bc-48da-890e-1ef543b83b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_cases = []\n",
    "for item in result:\n",
    "    test_case = LLMTestCase(\n",
    "        input=item['question'],\n",
    "        actual_output=item['answer'],\n",
    "        expected_output=item['ground_truths'],\n",
    "        retrieval_context=item['contexts']\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "scores = []\n",
    "for test_case in test_cases:\n",
    "    metric.measure(test_case)\n",
    "    scores.append(metric.score)\n",
    "\n",
    "average_score = sum(scores) / len(scores)\n",
    "\n",
    "print(\"Average Context precision Score:\", average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519aeb9-4b75-4ddb-b2a5-b5cbfa8cb347",
   "metadata": {},
   "source": [
    "### UPTRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d993e-1882-4843-a44b-2b14308c1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import EvalLLM, ResponseMatching\n",
    "data = []\n",
    "scores=[]\n",
    "eval_llm = EvalLLM(openai_api_key=openai_api_key)\n",
    "\n",
    "for item in result:\n",
    "    data = [{\n",
    "        \"question\": [item['question']],\n",
    "        \"response\": [item['answer']],\n",
    "        \"ground_truth\": [item['ground_truths']],\n",
    "        \n",
    "    }]\n",
    "    \n",
    "    res = eval_llm.evaluate(\n",
    "        data=data,\n",
    "        checks = [ResponseMatching(method = 'llm')]  \n",
    "    )\n",
    "   \n",
    "    for evaluation_result in res:\n",
    "        score = evaluation_result['score_response_match_llm']\n",
    "        scores.append(score)\n",
    "  \n",
    "average_score = sum(scores) / len(scores)\n",
    "print(\"Average Context precision Score:\", average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08e165-45a9-4786-82c7-57b4798338ea",
   "metadata": {},
   "source": [
    "### TRULENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc9998-d544-4832-8c09-c7f755870dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru, Feedback, Select\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "from trulens_eval import TruCustomApp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tru = Tru()\n",
    "provider = OpenAI()\n",
    "\n",
    "f_context_relevance = Feedback(provider.context_relevance_with_cot_reasons).on(Select.RecordCalls.retrieve.args.query).on(Select.RecordCalls.retrieve.rets).aggregate(np.mean)\n",
    "\n",
    "class DummyRAG:\n",
    "    def __init__(self, results):\n",
    "        self.results = results\n",
    "\n",
    "    @instrument\n",
    "    def retrieve(self, query):\n",
    "        for result in self.results:\n",
    "            if result['question'] == query:\n",
    "                return result['contexts']\n",
    "        return []\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query, contexts):\n",
    "        for result in self.results:\n",
    "            if result['question'] == query:\n",
    "                return result['answer']\n",
    "        return \"\"\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query):\n",
    "        contexts = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, contexts)\n",
    "        return completion\n",
    "\n",
    "dummy_rag = DummyRAG(result)\n",
    "tru_rag = TruCustomApp(dummy_rag, app_id='RAG v1', feedbacks=[f_context_relevance])\n",
    "\n",
    "with tru_rag as recording:\n",
    "    for i in result:\n",
    "        dummy_rag.query(i['question'])\n",
    "\n",
    "print(tru.get_leaderboard(app_ids=[\"RAG v1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7db3d-136a-4663-a1d8-dec3e179f409",
   "metadata": {},
   "source": [
    "### TONIC VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4de26b-a99d-4447-85f8-87a50a7fad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import Benchmark\n",
    "from tonic_validate import ValidateScorer\n",
    "from tonic_validate.metrics import AnswerConsistencyMetric, AnswerSimilarityMetric,AugmentationPrecisionMetric\n",
    "benchmark = Benchmark(\n",
    "    questions=[r['question'] for r in result],\n",
    "    answers=[r['ground_truths'] for r  in result]\n",
    ")\n",
    "\n",
    "def get_rag_response(question):\n",
    "    for i in result:\n",
    "        if i['question'] == question:\n",
    "            return {\n",
    "                \"llm_answer\": i['answer'],\n",
    "                \"llm_context_list\": i['contexts']\n",
    "            }\n",
    "\n",
    "\n",
    "scorer = ValidateScorer(metrics=[AnswerSimilarityMetric()])\n",
    "\n",
    "run = scorer.score(benchmark, get_rag_response)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661f384-b4b4-4022-841e-e69e33a80700",
   "metadata": {},
   "source": [
    "### RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a2759-4c27-45b5-be75-009640d92d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_recall\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset \n",
    "import pandas as pd\n",
    "dfresult = pd.DataFrame(result)\n",
    "dfresult['ground_truths'] = dfresult['ground_truths'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "dataset = Dataset.from_pandas(dfresult)\n",
    "score = evaluate(dataset,metrics=[context_recall])\n",
    "scoredf=score.to_pandas()\n",
    "avg_score = scoredf['context_recall'].mean()\n",
    "print(\"Average Score:\", avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff73da-b6e7-44b6-9086-ab93a8437253",
   "metadata": {},
   "source": [
    "### FALCON EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86347274-1577-4f1b-9c01-bd7a8f0e9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from falcon_evaluate.fevaluate_results import ModelScoreSummary\n",
    "from falcon_evaluate.fevaluate_plot import ModelPerformancePlotter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def convert_to_dataframe(results):\n",
    "    data = {\n",
    "        'prompt': [],\n",
    "        'reference': [],\n",
    "        'Model A': []\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        data['prompt'].append(result['question'])\n",
    "        data['reference'].append(result['ground_truths'])\n",
    "        data['Model A'].append(result['answer'])\n",
    "       \n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = convert_to_dataframe(result)\n",
    "model_score_summary = ModelScoreSummary(df)\n",
    "resultt,agg_score_df = model_score_summary.execute_summary()\n",
    "resultt.to_csv(\"falconresult.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
