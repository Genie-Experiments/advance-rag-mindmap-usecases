{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.vectorstores import Chroma\n",
    "from groq import Groq\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, SimpleNodeParser, get_leaf_nodes\n",
    "from llama_index.core import Prompt\n",
    "from llama_index.core import VectorStoreIndex,StorageContext, load_index_from_storage\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import openai\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from typing import Callable, Optional\n",
    "from llama_index.core.utils import globals_helper, get_tokenizer\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import os\n",
    "from langchain_community.document_loaders import DiffbotLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "       \"What are the steps required to set up a boilerplate project using the Emumba Plugin?\",\n",
    "        \"Can you list some of the key features provided by the Emumba Plugin for React applications?\",\n",
    "        \"What is the purpose of the generateFiles function in the project setup generator, and how does it use the options provided?\",\n",
    "        \"Describe the role of addDependenciesToPackageJson in the project setup process.\",\n",
    "        \"How does the ProjectSetupGeneratorSchema interface influence the behavior of the project setup generator?\",\n",
    "        \"Explain how the project configuration is added to the workspace using addProjectConfiguration in the context of the setup process.\",\n",
    "        \"Describe the process and the purpose of creating a test project in the beforeAll setup of the emumba-plugin tests.\",\n",
    "        \"How does the test for emumba-plugin ensure that the plugin is properly installed and functional within a generated project?\",\n",
    "        \"there's a function used to create a test project directory. Output the code snippet that showcases how this directory is created and initialized.\",\n",
    "        '''Given the following incomplete snippet, complete the function to add a specific dependency to the project's package.json. \n",
    "            Assume the function addDependenciesToPackageJson is already imported.\n",
    "            function enhancePackageJson(tree: Tree, projectName: string) {\n",
    "            // Add 'react-redux' as a dependency\n",
    "            addDependenciesToPackageJson(tree, projectName, {\n",
    "                'react-redux': '^7.2.0'\n",
    "            }, {});\n",
    "            // Complete the function to also add 'redux' as a dependency\n",
    "        }'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=\"your-key\"\n",
    "client = Groq(\n",
    "    api_key=\"your-key\",\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"your-key\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"your-key\"\n",
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name='llama3-8b-8192')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = \"./data/Boilerplate\"\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 1 : Metadata Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.page_content = content\n",
    "        self.metadata = metadata if metadata else {}\n",
    "    \n",
    "    def add_metadata(self, key, value):\n",
    "        self.metadata[key] = value\n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    # Example metadata collection\n",
    "                    metadata = {\n",
    "                        'file_name': file,\n",
    "                        'file_path': str(file_path),\n",
    "                        'file_size': os.path.getsize(file_path)\n",
    "                    }\n",
    "                    documents.append(Document(content, metadata))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "repo_path = \"./data/Boilerplate\"\n",
    "documents = read_files(repo_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "metadocs = text_splitter.split_documents(documents)\n",
    "texts = [d.page_content for d in metadocs]\n",
    "metadatas = [doc.metadata for doc in metadocs]\n",
    "\n",
    "if len(texts) != len(metadatas):\n",
    "    raise ValueError(f\"Number of texts ({len(texts)}) and metadata entries ({len(metadatas)}) do not match.\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), \n",
    "    metadatas=metadatas)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2-1 : Chunking - Character text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = \"./data/Boilerplate\"\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "docs=text_splitter.create_documents(docs_texts)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2-2 : Chunking : Recursive splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE REFER TO E0 IN THIS NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2-3 : Chunking : tiktoken based splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = \"./data/Boilerplate\"\n",
    "\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents(docs_texts)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2-4 : Chunking : Semantic based splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "repo_path = \"./data/Boilerplate\"\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "text_splitter = SemanticChunker(embeddings,breakpoint_threshold_type=\"percentile\")\n",
    "docs = text_splitter.create_documents(docs_texts)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3 : Hierarichal Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitRetrievedNodesLength:\n",
    "\n",
    "    def __init__(self, limit: int = 2500, tokenizer: Optional[Callable] = None):\n",
    "        self._tokenizer = tokenizer or get_tokenizer()\n",
    "\n",
    "        self.limit = limit\n",
    "\n",
    "    def postprocess_nodes(self, nodes, query_bundle):\n",
    "        included_nodes = []\n",
    "        current_length = 0\n",
    "\n",
    "        for node in nodes:\n",
    "            current_length += len(self._tokenizer(node.node.get_content(metadata_mode=MetadataMode.LLM)))\n",
    "            if current_length > self.limit:\n",
    "                break\n",
    "            included_nodes.append(node)\n",
    "\n",
    "        return included_nodes\n",
    "    \n",
    "LLAMA_PROMPT_TEMPLATE = (\n",
    " \"<s>[INST] <<SYS>>\"\n",
    " \"Use the following context to answer the user's question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    " \"<</SYS>>\"\n",
    " \"<s>[INST] Context: {context_str} Question: {query_str} Only return the helpful answer below and nothing else. Helpful answer:[/INST]\"\n",
    ")\n",
    "qa_template = Prompt(LLAMA_PROMPT_TEMPLATE)\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128],\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"./data/Boilerplate\"\n",
    "input_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if not file.startswith('.')]\n",
    "documents = SimpleDirectoryReader(input_files=input_files).load_data()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "leaf_nodes=get_leaf_nodes(nodes)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "directory=\"./llama_docs/docs/Boilerplate\"\n",
    "index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)\n",
    "index.storage_context.persist(persist_dir=f\"./data_{os.path.basename(directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E4 : Knwoledge Graph Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_chain(llm,retriever,queries):\n",
    "    rag_template = \"\"\"\n",
    "    The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "\n",
    "    Data Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": rag_prompt}\n",
    "    )\n",
    "\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "            response = qa({\"query\": query})\n",
    "            \n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            contexts = process_context(response['source_documents'])\n",
    "            result.append({'question': query, 'answer':response['result'], 'ground_truths': ground_truth,'contexts':contexts})\n",
    "\n",
    "           \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,retriever,queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
