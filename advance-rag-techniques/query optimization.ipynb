{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG OVER CODE - QUERY OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from pathlib import Path\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "import boto3\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema  import StrOutputParser\n",
    "from langchain.schema.runnable  import RunnablePassthrough\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain.chains import HypotheticalDocumentEmbedder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from groq import Groq\n",
    "import re\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "import getpass\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_cohere import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "queries = [\n",
    "       \"What are the steps required to set up a boilerplate project using the Emumba Plugin?\",\n",
    "        \"Can you list some of the key features provided by the Emumba Plugin for React applications?\",\n",
    "        \"What is the purpose of the generateFiles function in the project setup generator, and how does it use the options provided?\",\n",
    "        \"Describe the role of addDependenciesToPackageJson in the project setup process.\",\n",
    "        \"How does the ProjectSetupGeneratorSchema interface influence the behavior of the project setup generator?\",\n",
    "        \"Explain how the project configuration is added to the workspace using addProjectConfiguration in the context of the setup process.\",\n",
    "        \"Describe the process and the purpose of creating a test project in the beforeAll setup of the emumba-plugin tests.\",\n",
    "        \"How does the test for emumba-plugin ensure that the plugin is properly installed and functional within a generated project?\",\n",
    "        \"there's a function used to create a test project directory. Output the code snippet that showcases how this directory is created and initialized.\",\n",
    "        '''Given the following incomplete snippet, complete the function to add a specific dependency to the project's package.json. \n",
    "            Assume the function addDependenciesToPackageJson is already imported.\n",
    "            function enhancePackageJson(tree: Tree, projectName: string) {\n",
    "            // Add 'react-redux' as a dependency\n",
    "            addDependenciesToPackageJson(tree, projectName, {\n",
    "                'react-redux': '^7.2.0'\n",
    "            }, {});\n",
    "            // Complete the function to also add 'redux' as a dependency\n",
    "        }'''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Ids for llm using bedrock service "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=\"your-key\"\n",
    "llm = ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name='llama3-70b-8192',\n",
    "            temperature=0\n",
    "    )    \n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "client = Groq(\n",
    "    api_key=\"your-key\",\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"your-key\"\n",
    "os.environ[\"COHERE_API_KEY\"] =  \"your-key\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract content from Github Folder and Split using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'data/Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "documents = SimpleDirectoryReader('data/Boilerplate').load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP BACK PROMPTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system = \"\"\" we have a specific query then we firstly take a step back and try to get an answer for a generic query.\n",
    "let's say we ask a specific question why does the login button on my website doesn't work for some users. \n",
    "and then a step back query would be what's the functionality of login button on a website for multiple users. So, genearte a step back query, and also return original query, your output format should be =  <stepbackquery> <original query>, here is the original query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "step_back = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_chain(llm,retriever,queries,step_back):\n",
    "    result=[]\n",
    "    for i,query in enumerate(queries, start=0):\n",
    "        file_path = 'positive_ground_truths.xlsx'\n",
    "        df = pd.read_excel(file_path)\n",
    "        question = (query)\n",
    "        newqueries = step_back.invoke({\"question\": question})\n",
    "        rag_template = \"\"\"\n",
    "        The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "    \n",
    "        Data Context:\n",
    "        {context}\n",
    "    \n",
    "        Question: {question}\n",
    "    \n",
    "        Answer:\"\"\"\n",
    "    \n",
    "        def process_context(source_documents):\n",
    "            contexts = []\n",
    "            for doc in source_documents:\n",
    "                if hasattr(doc, 'page_content'):\n",
    "                    contexts.append(doc.page_content)\n",
    "                else:\n",
    "                    contexts.append(\"Invalid document format\")\n",
    "            return contexts\n",
    "    \n",
    "        rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": rag_prompt}\n",
    "        )\n",
    "        \n",
    "        response = qa({\"query\": newqueries})\n",
    "                \n",
    "        ground_truth = df.iloc[i]['ground truth']  \n",
    "        contexts = process_context(response['source_documents'])\n",
    "        result.append({'question': query, 'answer': response['result'], 'ground_truths': ground_truth,'contexts':contexts})\n",
    "    return result\n",
    "               \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,retriever,queries,step_back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system = \"\"\" we have a query, for that given query you have to generate a follow up question\n",
    "So, genearte a follow up question, and also return original query, your output format should be = <original query>,<Follow up query> here is the original query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "multi_step = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_QA_chain(llm,retriever,queries,multi_step):\n",
    "    for i,query in enumerate(queries, start=0):\n",
    "        \n",
    "        file_path = 'positive_ground_truths.xlsx'\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        newqueries = multi_step.invoke({\"question\": (query)})\n",
    "        print(newqueries)\n",
    "        rag_template = \"\"\"\n",
    "        The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "    \n",
    "        Data Context:\n",
    "        {context}\n",
    "    \n",
    "        Question: {question}\n",
    "    \n",
    "        Answer:\"\"\"\n",
    "    \n",
    "        def process_context(source_documents):\n",
    "            contexts = []\n",
    "            for doc in source_documents:\n",
    "                if hasattr(doc, 'page_content'):\n",
    "                    contexts.append(doc.page_content)\n",
    "                else:\n",
    "                    contexts.append(\"Invalid document format\")\n",
    "            return contexts\n",
    "    \n",
    "        rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": rag_prompt}\n",
    "        )\n",
    "        \n",
    "        response = qa({\"query\": newqueries})\n",
    "        \n",
    "        ground_truth = df.iloc[i]['ground truth']  \n",
    "        contexts = process_context(response['source_documents'])\n",
    "        result.append({'question': query, 'answer': response['result'], 'ground_truths': ground_truth,'contexts':contexts})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=multi_QA_chain(llm,retriever,queries,multi_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  QUERY EXPANSION-MULTIQUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_db = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "base_retriever = retriever.as_retriever(search_kwargs={\"k\":3})\n",
    "final_retriever = MultiQueryRetriever.from_llm(retriever, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_chain(llm,final_retriever,queries):\n",
    "    \n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "    \n",
    "    \n",
    "    tmpl = \"\"\"\n",
    "        You are an assistant to answer a question from user with a context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        The response should be presented as a list of key points, after creating the title of the content,\n",
    "            formatted in HTML with appropriate markup for clarity and organization.\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(tmpl)\n",
    "    result=[]\n",
    "    file_path = 'negative_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i,query in enumerate(queries):\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=final_retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "        response= qa({\"query\": query})\n",
    "        contexts = process_context(response['source_documents'])\n",
    "        ground_truth = df.iloc[i]['ground truth']\n",
    "        #result.append({'Query': query, 'Result': response['result'], 'GroundTruth': ground_truth, 'Context': contexts})\n",
    "        result.append({'question': query, 'answer': response['result'], 'ground_truths': ground_truth,'contexts':contexts})\n",
    "\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,final_retriever,queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  QUERY EXPANSION SUB QUERY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_chain(llm, retriever, queries):\n",
    "    prompt_template = \"\"\"\n",
    "    The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "\n",
    "    Data Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "\n",
    "    def generate_sub_queries(query):\n",
    "        sub_queries_prompt = f\"Given the complex query '{query}', please break it down into only 2 constituent subqueries. Each subquery should focus on a specific aspect or component of the original query. Ensure that each subquery is formulated clearly and can be processed independently. Also return sub queries in an array like this [subquery1 ,subquery2, subquery3]\"\n",
    "    \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": sub_queries_prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "        sub_queries = chat_completion.choices[0].message.content\n",
    "        \n",
    "        pattern = r\"\\[(.*?)]\"\n",
    "        matches = re.search(pattern, sub_queries, re.DOTALL)\n",
    "\n",
    "        if matches:\n",
    "            sub_queries_list = [sub_query.strip() for sub_query in matches.group(1).split(',')]\n",
    "            return sub_queries_list\n",
    "        else:\n",
    "            return [query]\n",
    "            \n",
    "            \n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    file_path = 'negative_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0):\n",
    "        sub_queries = generate_sub_queries(query)\n",
    "    \n",
    "        all_contexts = []\n",
    "\n",
    "        for sub_query in sub_queries:\n",
    "            response = qa({\"query\": sub_query})\n",
    "            contexts = process_context(response['source_documents'])\n",
    "            all_contexts.extend(contexts) \n",
    "\n",
    "        final_context = \" \".join(all_contexts) \n",
    "        final_prompt=f\"For given query : {query}, find answer for it from following context ; {final_context}\"\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "        final_response = chat_completion.choices[0].message.content\n",
    "\n",
    "        ground_truth = df.iloc[i]['ground truth']\n",
    "        \n",
    "        result.append({'question': query, 'answer': final_response, 'ground_truths': ground_truth,'contexts':all_contexts})\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,retriever,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embeddings (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = CohereEmbeddings()\n",
    "documents = SimpleDirectoryReader(\"./data/Boilerplate\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyderetriver(queries,query_engine):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        ground_truth = df.iloc[i]['ground truth']  \n",
    "        response = hyde_query_engine.query(query)\n",
    "        query_bundle = hyde(query)\n",
    "        hyde_doc = query_bundle.embedding_strs\n",
    "        result.append({'question': query, 'answer': str(response),'contexts':hyde_doc, 'ground_truths': ground_truth})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=hyderetriver(queries,query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY-REWRITING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def QA_chain(llm,retreiver,questions):\n",
    "    def _parse(text):\n",
    "        return text.strip('\"').strip(\"**\")\n",
    "\n",
    "    template = \"\"\"Provide a better search query for \\\n",
    "    search engine to answer the given question, end \\\n",
    "    the queries with ’**’. Question: \\\n",
    "    {x} Answer:\"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rewriter = rewrite_prompt | llm | StrOutputParser() | _parse\n",
    "    rag_template = \"\"\"\n",
    "    The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "\n",
    "    Data Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": rag_prompt}\n",
    "    )\n",
    "   \n",
    "    result = []\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(questions, start=0):\n",
    "        rewrittenquery=rewriter.invoke({\"x\": query})\n",
    "        response = qa({\"query\": rewrittenquery})\n",
    "        print(response)\n",
    "        contexts = process_context(response['source_documents'])\n",
    "        ground_truth = df.iloc[i]['ground truth']\n",
    "        result.append({\n",
    "            'Query': query,\n",
    "            'Result': response['result'],\n",
    "            'GroundTruth': ground_truth,\n",
    "            'Context': contexts\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,retriever,queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
