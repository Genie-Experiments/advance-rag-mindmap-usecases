{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.vectorstores.deeplake import DeepLake\n",
    "from langchain.vectorstores.deeplake import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.vectorstores import Chroma\n",
    "import pandas as pd\n",
    "import os\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "import os\n",
    "from pathlib import Path\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from groq import Groq\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "import requests\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from llama_index.postprocessor.jinaai_rerank import JinaRerank\n",
    "from llama_index.core.postprocessor import LongContextReorder\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "from numpy.linalg import norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries = [\"What are the steps required to set up a boilerplate project using the Emumba Plugin?\"]\n",
    "queries = [\n",
    "       \"What are the steps required to set up a boilerplate project using the Emumba Plugin?\",\n",
    "        \"Can you list some of the key features provided by the Emumba Plugin for React applications?\",\n",
    "        \"What is the purpose of the generateFiles function in the project setup generator, and how does it use the options provided?\",\n",
    "        \"Describe the role of addDependenciesToPackageJson in the project setup process.\",\n",
    "        \"How does the ProjectSetupGeneratorSchema interface influence the behavior of the project setup generator?\",\n",
    "        \"Explain how the project configuration is added to the workspace using addProjectConfiguration in the context of the setup process.\",\n",
    "        \"Describe the process and the purpose of creating a test project in the beforeAll setup of the emumba-plugin tests.\",\n",
    "        \"How does the test for emumba-plugin ensure that the plugin is properly installed and functional within a generated project?\",\n",
    "        \"there's a function used to create a test project directory. Output the code snippet that showcases how this directory is created and initialized.\",\n",
    "        '''Given the following incomplete snippet, complete the function to add a specific dependency to the project's package.json. \n",
    "            Assume the function addDependenciesToPackageJson is already imported.\n",
    "            function enhancePackageJson(tree: Tree, projectName: string) {\n",
    "            // Add 'react-redux' as a dependency\n",
    "            addDependenciesToPackageJson(tree, projectName, {\n",
    "                'react-redux': '^7.2.0'\n",
    "            }, {});\n",
    "            // Complete the function to also add 'redux' as a dependency\n",
    "        }'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=\"your-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"your-key\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"your-key\"\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name='llama3-70b-8192')    \n",
    "client = Groq(api_key=\"your-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colbert Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data/Boilerplate').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=4,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=6,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def colbertreranker(queries,query_engine):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        ground_truth = df.iloc[i]['ground truth']  \n",
    "        response = query_engine.query(query)\n",
    "        context=[]\n",
    "        for node in response.source_nodes:\n",
    "            context.append(node.node.get_content())\n",
    "        result.append({'question': query, 'answer': str(response), 'ground_truths': ground_truth,'contexts':context})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=colbertreranker(queries,query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'data/Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = CohereEmbeddings()\n",
    "docstore = DeepLake(\n",
    "    dataset_path=\"deeplake_vstore\",\n",
    "    embedding=embedding,\n",
    "    verbose=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "_ = docstore.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cohere_rerank = CohereRerank(\n",
    "    cohere_api_key=\"your-key\",\n",
    "    model=\"rerank-english-v3.0\",\n",
    ")\n",
    "\n",
    "docstore = DeepLake(\n",
    "    dataset_path=\"deeplake_vstore\",\n",
    "    embedding=embedding,\n",
    "    verbose=False,\n",
    "    read_only=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank,\n",
    "    \n",
    "    base_retriever=docstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"fetch_k\": 4,\n",
    "            \"k\": 6,\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cohere_chain(llm,compression_retriever,queries):\n",
    "    prompt_template = \"\"\"You are an intelligent chatbot that can answer user's \n",
    "    queries. You will be provided with Relevant context based on the user's queries. \n",
    "    Your task is to analyze user's query and generate response for the query \n",
    "    utiliing the context. \n",
    "    \n",
    "    NEVER generate response to queries for which there is no or irrelevant context.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": False}\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        print(source_documents)\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "            response = qa.invoke({\"query\": query})\n",
    "            \n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            contexts = process_context(response['source_documents'])\n",
    "            result.append({'question': query, 'answer': response['result'], 'ground_truths': ground_truth,'contexts': contexts})\n",
    "           \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result=cohere_chain(llm,compression_retriever,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data/Boilerplate').load_data()\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name='llama3-70b-8192')    \n",
    "embedding=OllamaEmbeddings(model='mxbai-embed-large')\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GEN_PROMPT = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "vector_retriever = index.as_retriever(similarity_top_k=6)\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=4\n",
    ")\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=1,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    query_gen_prompt=QUERY_GEN_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragfusion(client,retriever,queries):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        \n",
    "            nodes_with_scores = retriever.retrieve(query)\n",
    "            context=[]\n",
    "            for node in nodes_with_scores:\n",
    "                print(node)\n",
    "                context.append(node.text)\n",
    "            prompt= f\"\"\"Answer the question based only on the following context:{context}Question: {query}\"\"\"\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model='llama3-70b-8192',\n",
    "            )\n",
    "            response=(chat_completion.choices[0].message.content)\n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            result.append({'question': query, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=ragfusion(client,retriever,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANKGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data/Boilerplate').load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "llm = Ollama(model=\"llama3\", request_timeout=1200.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_nodes(\n",
    "    query_str, vector_top_k=6, reranker_top_n=4, with_reranker=True\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        print(\"hi\")\n",
    "        reranker = RankGPTRerank(\n",
    "            llm=OpenAI(\n",
    "                model=\"gpt-3.5-turbo-16k\",\n",
    "                temperature=0.0,\n",
    "                api_key=\"your-key\",\n",
    "            ),\n",
    "            top_n=reranker_top_n,\n",
    "            verbose=True,\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "    return retrieved_nodes\n",
    "def rankthroughgpt(client,queries):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        \n",
    "            new_nodes = get_retrieved_nodes(\n",
    "                query,\n",
    "                vector_top_k=6,\n",
    "                reranker_top_n=4,\n",
    "                with_reranker=True,\n",
    "            )\n",
    "            context=[]\n",
    "            for node in new_nodes:\n",
    "                print(new_nodes)\n",
    "                context.append(node.text)\n",
    "                \n",
    "            prompt= f\"\"\"Answer the question based only on the following context:{context}Question: {query}\"\"\"\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model='llama3-70b-8192',\n",
    "            )\n",
    "            print(\"kk\")\n",
    "            response=(chat_completion.choices[0].message.content)\n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            result.append({'question': query, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=rankthroughgpt(client,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jina AI reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data/Boilerplate').load_data()\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_api_key = \"your-key\"\n",
    "jina_embeddings = JinaEmbedding(api_key=jina_api_key)\n",
    "jina_rerank = JinaRerank(api_key=jina_api_key, top_n=4)\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[jina_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jinarerank(client,query_engine,queries):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "            response = query_engine.query(query)\n",
    "            context=[]\n",
    "            scores=[]\n",
    "            for node in response.source_nodes:\n",
    "                text = node.text\n",
    "                context.append(node.text)\n",
    "                scores.append(node.score)\n",
    "            prompt= f\"\"\"Answer the question based only on the following context:{context}Question: {query}\"\"\"\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model='llama3-70b-8192',\n",
    "            )\n",
    "            response=(chat_completion.choices[0].message.content)\n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            result.append({'question': query, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "    return result,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result,scores=jinarerank(client,query_engine,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long context reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'data/Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "retriever = Chroma.from_texts(docs_texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context(contexts,source_documents):\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longcontextreorder(retriever,queries):\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        docs = retriever.invoke(query)\n",
    "        reordering = LongContextReorder()\n",
    "        reordered_docs = reordering.transform_documents(docs)\n",
    "        contexts=[]\n",
    "        contexts = process_context(contexts,reordered_docs)\n",
    "        document_prompt = PromptTemplate(\n",
    "            input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    "        )\n",
    "        document_variable_name = \"context\"\n",
    "        \n",
    "        stuff_prompt_override = \"\"\"Given this text extracts:\n",
    "        -----\n",
    "        {context}\n",
    "        -----\n",
    "        Please answer the following question:\n",
    "        {query}\"\"\"\n",
    "        \n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    "        )\n",
    "        \n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        chain = StuffDocumentsChain(\n",
    "            llm_chain=llm_chain,\n",
    "            document_prompt=document_prompt,\n",
    "            document_variable_name=document_variable_name,\n",
    "        )\n",
    "        response=chain.run(input_documents=reordered_docs, query=query)\n",
    "        ground_truth = df.iloc[i]['ground truth']  \n",
    "        result.append({'question': query, 'answer': response, 'ground_truths': ground_truth,'contexts': contexts})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result=longcontextreorder(retriever,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGE reranker   (run this on gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {}\n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagTokenForGeneration\n",
    "from FlagEmbedding import BGEM3FlagModel, FlagReranker\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained('facebook/rag-token-nq')\n",
    "model = RagTokenForGeneration.from_pretrained('facebook/rag-token-nq')\n",
    "model1 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "groq_api_key = \"your-key\"\n",
    "client = Groq(api_key=groq_api_key)\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='llama3-70b-8192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def bge_m3_embed(query: str):\n",
    "    embeddings = model1.encode([query])['dense_vecs'][0]\n",
    "    return embeddings\n",
    "\n",
    "def embed_docs(docs):\n",
    "    contents = [doc.page_content for doc in docs]\n",
    "    embeddings = np.array([bge_m3_embed(content) for content in contents])\n",
    "    return embeddings\n",
    "embeddings = embed_docs(docs)\n",
    "index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank_documents(query, index, docs, k=5, top_n=4):\n",
    "    query_embedding = bge_m3_embed(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_docs = [docs[idx] for idx in indices.flatten()]\n",
    "\n",
    "    passages = [doc.page_content for doc in retrieved_docs]\n",
    "    query_passage_pairs = [[query, passage] for passage in passages]\n",
    "    scores = reranker.compute_score(query_passage_pairs)\n",
    "    print(scores)\n",
    "    scored_docs = list(zip(retrieved_docs, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for doc, score in scored_docs[:top_n]]\n",
    "    return top_docs\n",
    "\n",
    "def rag_answer(questions, index, docs, top_n=4):\n",
    "    result = []\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        top_docs = retrieve_and_rerank_documents(question, index, docs, top_n=top_n)\n",
    "        contexts = [doc.page_content for doc in top_docs]  # keep as list\n",
    "\n",
    "        prompt = f\"\"\"Answer the question based only on the following context: {' '.join(contexts)} Question: {question}\"\"\"\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model='llama3-70b-8192',\n",
    "        )\n",
    "        response = chat_completion.choices[0].message.content\n",
    "\n",
    "        ground_truth = df.iloc[i]['ground truth']\n",
    "        result.append({'question': question, 'answer': response, 'ground_truths': ground_truth, 'contexts': contexts})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "result = rag_answer(queries, index, docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
