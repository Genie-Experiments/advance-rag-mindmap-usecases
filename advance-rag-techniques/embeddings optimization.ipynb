{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from groq import Groq\n",
    "import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import voyageai\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from langchain_cohere import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "       \"What are the steps required to set up a boilerplate project using the Emumba Plugin?\",\n",
    "        \"Can you list some of the key features provided by the Emumba Plugin for React applications?\",\n",
    "        \"What is the purpose of the generateFiles function in the project setup generator, and how does it use the options provided?\",\n",
    "        \"Describe the role of addDependenciesToPackageJson in the project setup process.\",\n",
    "        \"How does the ProjectSetupGeneratorSchema interface influence the behavior of the project setup generator?\",\n",
    "        \"Explain how the project configuration is added to the workspace using addProjectConfiguration in the context of the setup process.\",\n",
    "        \"Describe the process and the purpose of creating a test project in the beforeAll setup of the emumba-plugin tests.\",\n",
    "        \"How does the test for emumba-plugin ensure that the plugin is properly installed and functional within a generated project?\",\n",
    "        \"there's a function used to create a test project directory. Output the code snippet that showcases how this directory is created and initialized.\",\n",
    "        '''Given the following incomplete snippet, complete the function to add a specific dependency to the project's package.json. \n",
    "            Assume the function addDependenciesToPackageJson is already imported.\n",
    "            function enhancePackageJson(tree: Tree, projectName: string) {\n",
    "            // Add 'react-redux' as a dependency\n",
    "            addDependenciesToPackageJson(tree, projectName, {\n",
    "                'react-redux': '^7.2.0'\n",
    "            }, {});\n",
    "            // Complete the function to also add 'redux' as a dependency\n",
    "        }'''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {} \n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content)) \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'data/Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=\"your-key\"\n",
    "groq_api_key=\"your-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your-key\"\n",
    "os.environ[\"VOYAGE_API_KEY\"]=\"your-key\"\n",
    "os.environ[\"COHERE_API_KEY\"]=\"your-key\"\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name='llama3-70b-8192')    \n",
    "client = Groq(api_key=\"your-key\")\n",
    "jinaai_api_key=\"your-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store : Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_embeddings = CohereEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"chroma_embeds\",\n",
    "    embedding=cohere_embeddings,\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store : Ollama Embeddings : mxbai-embed-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OllamaEmbeddings(model='mxbai-embed-large')\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"ollama_embeds_mxbai\",\n",
    "    embedding=embedding,\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store : Ollama Embeddings : all-minillm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"ollama_embeds_minillm\",\n",
    "    embedding=OllamaEmbeddings(model='all-minilm'),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store : Openai Embeddings : text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"openai_embeds\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store : Voyage Embedding : Large-2-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"voyage_embeds\",\n",
    "    embedding=VoyageAIEmbeddings(\n",
    "    voyage_api_key=\"pa-zfj3RRNPt0KXNoESdHmERG_HM-zel5LdHuVLteV063s\", model=\"voyage-law-2\"\n",
    "),\n",
    ")        \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store :  Jina-AI  : Has its Own pipeline!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data/Boilerplate\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, embed_model=JinaEmbedding(\n",
    "    api_key=jinaai_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")\n",
    ")\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jina_rag(client,retriever,queries):\n",
    "    result=[]\n",
    "    file_path = 'ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "        \n",
    "            search_query_retrieved_nodes = retriever.retrieve(query)\n",
    "            \n",
    "            context=[]\n",
    "            for n in search_query_retrieved_nodes:\n",
    "                context.append(n)\n",
    "            prompt= f\"\"\"Answer the question based only on the following context:{context}Question: {query}\"\"\"\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model='llama3-70b-8192',\n",
    "            )\n",
    "            response=(chat_completion.choices[0].message.content)\n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            result.append({'question': query, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=jina_rag(client,retriever,queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def QA_chain(llm,retriever,queries):\n",
    "    rag_template = \"\"\"\n",
    "    The following data comes from various files in a GitHub repository, which may contain information of any file extension. Your task is to search for an answer to a specific question within this data. Do not attempt to create an answer on your own. If you cannot find any reference to the query within the provided data, simply respond with, \"There is no such reference to this.\"\n",
    "\n",
    "    Data Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    def process_context(source_documents):\n",
    "        contexts = []\n",
    "        for doc in source_documents:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                contexts.append(doc.page_content)\n",
    "            else:\n",
    "                contexts.append(\"Invalid document format\")\n",
    "        return contexts\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": rag_prompt}\n",
    "    )\n",
    "\n",
    "    result=[]\n",
    "    file_path = 'positive_ground_truths.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    for i, query in enumerate(queries, start=0): \n",
    "            time.sleep(6)\n",
    "            response = qa({\"query\": query})\n",
    "            \n",
    "            ground_truth = df.iloc[i]['ground truth']  \n",
    "            contexts = process_context(response['source_documents'])\n",
    "            result.append({'question': query, 'answer': response['result'], 'ground_truths': ground_truth,'contexts': contexts})\n",
    "           \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=QA_chain(llm,retriever,queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGE-M3 (Run on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content):\n",
    "        self.page_content = content\n",
    "        self.metadata = {}\n",
    "\n",
    "def read_files(directory_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(Document(content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "repo_path = 'Boilerplate'\n",
    "documents = read_files(repo_path)\n",
    "docs_texts = [d.page_content for d in documents]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagTokenForGeneration\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained('facebook/rag-token-nq')\n",
    "model = RagTokenForGeneration.from_pretrained('facebook/rag-token-nq')\n",
    "model1 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "def create_faiss_index(embeddings):\n",
    "    # Create and return a FAISS index for the given embeddings\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def bge_m3_embed(query: str):\n",
    "    embeddings = model1.encode([query])['dense_vecs'][0]\n",
    "    return embeddings\n",
    "\n",
    "def embed_docs(docs):\n",
    "    contents = [doc.page_content for doc in docs]\n",
    "    embeddings = np.array([bge_m3_embed(content) for content in contents])\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_docs(docs)\n",
    "index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index, k=5):\n",
    "    # Retrieve k most similar documents for a given query\n",
    "    query_embedding = bge_m3_embed(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return indices.flatten()\n",
    "\n",
    "def rag_answer(questions, index,docs):\n",
    "  for i, question in enumerate(questions, start=0):\n",
    "      context=[]\n",
    "      retrieved_indices = retrieve_documents(question, index)\n",
    "      retrieved_docs = [docs[idx] for idx in retrieved_indices]  # Adjust based on your docs structure\n",
    "      contexts = [doc.page_content for doc in docs]  # keep as list\n",
    "      result=[]\n",
    "      file_path = '/content/positive_ground_truths.xlsx'\n",
    "      df = pd.read_excel(file_path)\n",
    "      prompt= f\"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"\n",
    "      chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "           {\n",
    "                          \"role\": \"user\",\n",
    "                          \"content\": prompt,\n",
    "                      }\n",
    "                  ],\n",
    "                  model='llama3-70b-8192',\n",
    "       )\n",
    "      response=(chat_completion.choices[0].message.content)\n",
    "      print(df.iloc[i]['ground truth']  )\n",
    "      ground_truth = df.iloc[i]['ground truth']\n",
    "      result.append({'question': question, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "  return result\n",
    "\n",
    "result=rag_answer(queries, index,docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM EMBEDDER (Run on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import LLMEmbedder\n",
    "model1 = LLMEmbedder('BAAI/llm-embedder', use_fp16=False)\n",
    "tokenizer = RagTokenizer.from_pretrained('facebook/rag-token-nq')\n",
    "model = RagTokenForGeneration.from_pretrained('facebook/rag-token-nq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "task = \"qa\"\n",
    "def bge_m3_embed(query: str):\n",
    "    #embeddings = model1.encode([query])['dense_vecs'][0]\n",
    "    key_embeddings = model1.encode_keys(query, task=task)\n",
    "    return embeddings\n",
    "\n",
    "def embed_docs(docs):\n",
    "    contents = [doc.page_content for doc in docs]\n",
    "    embeddings = np.array([bge_m3_embed(content) for content in contents])\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_docs(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "new_embeddings = embeddings.reshape(-1, embeddings.shape[-1])\n",
    "index = create_faiss_index(new_embeddings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index, k=2):\n",
    "    # Retrieve k most similar documents for a given query\n",
    "    query_embedding = bge_m3_embed(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index, k=5):\n",
    "    query_embedding = bge_m3_embed(query).reshape(-1, embeddings.shape[-1])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return indices.flatten()\n",
    "\n",
    "def rag_answer(questions, index, docs):\n",
    "    result = []\n",
    "    for i, question in enumerate(questions, start=0):\n",
    "        retrieved_indices = retrieve_documents(question, index)\n",
    "        wrapped_indices = [idx % len(docs) for idx in retrieved_indices]\n",
    "        retrieved_docs = [docs[idx] for idx in wrapped_indices]\n",
    "        context=retrieved_docs\n",
    "        file_path = '/content/positive_ground_truths.xlsx'\n",
    "        df = pd.read_excel(file_path)\n",
    "        prompt = f\"\"\"Answer the question based only on the following context:{context} Question: {question}\"\"\"\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            model='llama3-70b-8192',\n",
    "        )\n",
    "        response = (chat_completion.choices[0].message.content)\n",
    "        print(response)\n",
    "        ground_truth = df.iloc[i]['ground truth']\n",
    "        print(df.iloc[i]['ground truth'])\n",
    "        result.append({'question': question, 'answer': response, 'ground_truths': ground_truth,'contexts': context})\n",
    "\n",
    "    return result\n",
    "\n",
    "result=rag_answer(queries, index,docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
